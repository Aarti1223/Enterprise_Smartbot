# ğŸ§  Final RAG Backend with LangChain + Pinecone + HuggingFace

This is a **Retrieval-Augmented Generation (RAG)** backend that integrates:

- âœ… LangChain framework
- âœ… HuggingFace sentence embeddings
- âœ… Pinecone Vector DB
- âœ… OpenAI GPT-based reasoning
- âœ… SQLite for record storage

---

## ğŸš€ Features

- ğŸ” Embed `.docx`/text content using `all-MiniLM-L6-v2`
- ğŸ”— Store & query vectorized data via Pinecone
- ğŸ§  Use GPT for intelligent document Q&A
- ğŸ—ƒï¸ Save extracted records in SQLite and JSON
- âœ… Supports chunking + multi-query input

---

## ğŸ“ Project Structure

```bash
backend/
â”‚
â”œâ”€â”€ Final_rag.py           # Main RAG execution script
â”œâ”€â”€ backend.py             # Reusable backend utilities
â”œâ”€â”€ app.py                 # Flask/streamlit (if used)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env                   # Environment variables (not committed)
â”‚
â”œâ”€â”€ chunks.pkl             # Embedded content chunks
â”œâ”€â”€ combined_data.json     # Combined processed data
â”œâ”€â”€ embedded_chunks.json   # Embedding reference
â”œâ”€â”€ token_usage_log.json   # Usage log (if using OpenAI metering)
â”œâ”€â”€ travel_records.json    # Extracted records (example)
â”œâ”€â”€ upload_done.flag       # Upload tracking
â”œâ”€â”€ users.db               # SQLite DB
â””â”€â”€ ...
```
